{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install langchain\n",
    "# !pip install python-dotenv\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb\n",
    "# !pip install sentence-transformers\n",
    "# !pip install openai\n",
    "# !pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API token * key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] \n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "# print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceBgeEmbeddings,\n",
    ")\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "def find_text_by_red_strikethrough_status(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    strikethrough_texts = []\n",
    "    non_strikethrough_texts = []\n",
    "    full_texts = []\n",
    "\n",
    "    for page_number in range(len(document)):\n",
    "        page = document[page_number]\n",
    "        words = page.get_text(\"words\")  # 단어와 그 위치를 반환\n",
    "        paths = page.get_drawings()  # 페이지의 그래픽 요소를 추출\n",
    "\n",
    "        strikethrough_lines = []\n",
    "\n",
    "        # 그림 요소 중에서 선과 사각형을 검사하여 빨간색 취소선으로 판단\n",
    "        for path in paths:\n",
    "            color = path[\"color\"]\n",
    "            # 선의 색상이 빨간색인 경우에만 처리\n",
    "            if color == (1, 0, 0):  # RGB 색상으로 빨간색 확인\n",
    "                for item in path[\"items\"]:\n",
    "                    if item[0] == \"l\":  # 선인 경우\n",
    "                        p1, p2 = item[1:]\n",
    "                        if p1.y == p2.y:  # 수평선이면\n",
    "                            rect = fitz.Rect(p1.x, p1.y - 1, p2.x, p2.y + 1)\n",
    "                            strikethrough_lines.append(rect)\n",
    "                    elif item[0] == \"re\":  # 사각형인 경우\n",
    "                        rect = item[1]\n",
    "                        if rect.width > rect.height and rect.height < 3:  # 넓이가 높이보다 많이 크고 높이가 3pt 이하이면\n",
    "                            strikethrough_lines.append(rect)\n",
    "\n",
    "        # 각 단어와 취소선이 겹치는지 검사\n",
    "        same_line = words[0][5]\n",
    "        previous_strike = False\n",
    "        strike_line = ''\n",
    "        line = ''\n",
    "        for word in words:\n",
    "            word_rect = fitz.Rect(word[:4])  # 단어의 위치\n",
    "            strikethrough_found = False\n",
    "            for line_rect in strikethrough_lines:\n",
    "                if word_rect.intersects(line_rect):  # 겹치면\n",
    "                    strikethrough_found = True\n",
    "                    break\n",
    "            if not strikethrough_found:  # 취소선이 없으면\n",
    "                non_strikethrough_texts.append(word[4:6])  # 취소선이 적용되지 않은 단어 추가\n",
    "                if same_line != word[5]:\n",
    "                    same_line = word[5]\n",
    "                    line += '\\n'\n",
    "\n",
    "                line = line + ' ' + word[4]\n",
    "                \n",
    "                if strikethrough_found != previous_strike:\n",
    "                    full_texts.append('<del>' + strike_line + '<>')\n",
    "                    strike_line = ''\n",
    "                previous_strike = False\n",
    "            else:\n",
    "                strikethrough_texts.append(word[4:6])  # 취소선이 적용된 단어 추가\n",
    "                strike_line = strike_line  + ' ' + word[4]\n",
    "                if strikethrough_found != previous_strike:\n",
    "                    full_texts.append(line + '\\n')\n",
    "                    line=''\n",
    "                previous_strike = True\n",
    "        full_texts.append(line)\n",
    "    document.close()\n",
    "    return strikethrough_texts, non_strikethrough_texts, full_texts\n",
    "\n",
    "def re_text(full_texts):\n",
    "    full = ''\n",
    "    for text in full_texts:\n",
    "        cleaned_text = re.sub(r'\\.\\s*\\.', '', text)\n",
    "        full += cleaned_text\n",
    "    return full\n",
    "\n",
    "pdf_path = ['real_data_ex.pdf', 'real_data_ex2.pdf']\n",
    "docs = []\n",
    "for pdf in pdf_path:\n",
    "    strikethrough_texts, non_strikethrough_texts, full_texts = find_text_by_red_strikethrough_status(pdf)\n",
    "    texts = re_text(full_texts)\n",
    "    docs.append((pdf, texts))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "data = []\n",
    "for pdf, texts in docs:\n",
    "    split_texts = text_splitter.split_text(texts)\n",
    "    metadata = [{'source': pdf}] * len(split_texts)\n",
    "    pages = text_splitter.create_documents(split_texts, metadatas=metadata)\n",
    "    data.append(pages)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store data to vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings.sentence_transformer import (\n",
    "#     SentenceTransformerEmbeddings,\n",
    "# )\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# # directory = 'chroma_store_hugging'\n",
    "# directory = 'chroma_store_open'\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings()\n",
    "# # embeddings_model = HuggingFaceEmbeddings(\n",
    "# #     model_name='jhgan/ko-sbert-nli',\n",
    "# #     model_kwargs={'device':'cpu'},\n",
    "# #     encode_kwargs={'normalize_embeddings':True},\n",
    "# # )\n",
    "\n",
    "# embeddings_open = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# for pages in data:\n",
    "#     vector_index = Chroma.from_documents(\n",
    "#         pages, # Documents\n",
    "#         embedding = embeddings_open, # Text embedding model\n",
    "#         persist_directory=directory # persists the vectors to the file system\n",
    "#         )\n",
    "# # vector_index.persist()\n",
    "# print('count: ', vector_index._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 정보 삭제\n",
    "# ids = vector_index.get(0)['ids']\n",
    "\n",
    "# print('before: ', vector_index._collection.count())\n",
    "# for i in ids:\n",
    "#     vector_index._collection.delete(ids=i)\n",
    "# print('after :', vector_index._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  11\n"
     ]
    }
   ],
   "source": [
    "embeddings_open = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_index = Chroma(persist_directory='chroma_store_open', embedding_function=embeddings_open)\n",
    "print('count: ', vector_index._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(\n",
    "    search_type=\"similarity\", # Cosine Similarity\n",
    "    search_kwargs={\n",
    "        \"k\": 5, # Select top k search results\n",
    "    } \n",
    ")\n",
    "retriever.get_relevant_documents(\"루나서클의 회사의 상호, 상호의 영어명, 본점, 발행한 주식의 총수, 발행할 주식의 총수, 액면가, 회사성립연월일, 등기번호, 등록번호에 대해 표 형식으로 알려줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface model (no prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\chan\\.cache\\huggingface\\token\n",
      "Login successful\n",
      " | 회사의 상호 | 회사의 영어명 | 본점 | 발행한 주식의 총수 | 발행할 주식의 총수 | 액면가 | 회사성립연월일 | 등기번호 | 등록번호 |\n",
      "|---|---|---|---|---|---|---|---|---|\n",
      "| 마크앤컴퍼니 | Mark & Company Inc. | 서울특별시 서초구 강남대로 311, 705호(서초동, 드림플러스 강남) | 240,000 주 | 1,000,000 주 | 5,000 원 | 2019년 03월 11일 | 703850 | 110111-7038501 |</s>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '마크앤컴퍼니의 회사의 상호, 상호의 영어명, 본점, 발행한 주식의 총수, 발행할 주식의 총수, 액면가, 회사성립연월일, 등기번호, 등록번호에 대해 표 형식으로 정리해줘',\n",
       " 'result': ' | 회사의 상호 | 회사의 영어명 | 본점 | 발행한 주식의 총수 | 발행할 주식의 총수 | 액면가 | 회사성립연월일 | 등기번호 | 등록번호 |\\n|---|---|---|---|---|---|---|---|---|\\n| 마크앤컴퍼니 | Mark & Company Inc. | 서울특별시 서초구 강남대로 311, 705호(서초동, 드림플러스 강남) | 240,000 주 | 1,000,000 주 | 5,000 원 | 2019년 03월 11일 | 703850 | 110111-7038501 |</s>',\n",
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_new_tokens=2048,  \n",
    "    temperature=0.1, \n",
    "    callbacks=[StreamingStdOutCallbackHandler()], \n",
    "    streaming=True,  \n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    # chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "qa_chain.invoke('마크앤컴퍼니의 회사의 상호, 상호의 영어명, 본점, 발행한 주식의 총수, 발행할 주식의 총수, 액면가, 회사성립연월일, 등기번호, 등록번호에 대해 표 형식으로 정리해줘')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface model (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: | 항목 | 내용 |\n",
      "| --- | --- |\n",
      "| 상호 | 마크앤컴퍼니 |\n",
      "| 영어명 | Mark & Company Inc. |\n",
      "| 본점 | 서울특별시 서초구 강남대로 311, 705호(서초동, 드림플러스 강남) |\n",
      "| 액면가 | 금 500 원 (2019.06.10 변경) |\n",
      "| 발행한 주식의 총수 | 240,000 주 (2020.04.30 변경) |\n",
      "| 발행할 주식의 총수 | 1,000,000 주 |\n",
      "| 회사성립연월일 | 2019 년 03 월 11 일 |\n",
      "| 등기번호 | 703850 |\n",
      "| 등록번호 | 110111-7038501 |</s>\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "prompt_ex = '''You must answer the information I request. Don't guess the answer you don't know\n",
    "        In document, there is a text in parentheses called \"말소기록()\", which means that it is a deleted record.\n",
    "        Use the expiration record only if I request it and ignore the rest of the cases.\n",
    "        If metadata is different, it means it's a different document. \n",
    "        I want to get information about the same document with metadata.'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    you must not use information in the form of <del><> except when requested\n",
    "    The metadata being different means the documents are different. \n",
    "    I want to obtain information seperately for each document.\n",
    "    you answer me only using context for question:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {question}\n",
    "    i want korean answer and don't print english\"\"\")\n",
    "\n",
    "prompt_korean = ChatPromptTemplate.from_template(\"\"\"\n",
    "    너는 문서를 보고 대답을 하는 전문가야 모르는 답은 모른다고 답을 해줘\n",
    "    문서를 보면 \"말소기록()\" 괄호로 묶인 텍스트가 있는데 이거는 말소된 기록이란 뜻이야.\n",
    "    내가 말소기록을 요청할 경우에만 말소기록()을 사용하고 나머지 경우는 사용하지 마. \n",
    "    metadata가 다르면 다른 문서라는 의미야. 나는 메타데이터가 같은 문서에 대해 정보를 얻고 싶어\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\"\"\")\n",
    "\n",
    "retriever = vector_index.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\n",
    "        \"k\": 5, \n",
    "    } \n",
    ")\n",
    "\n",
    "# open_llm = ChatOpenAI(\n",
    "#     temperature=0.5,  # 창의성 (0.0 ~ 2.0)\n",
    "#     max_tokens=2048,  # 최대 토큰수\n",
    "#     model_name=\"gpt-3.5-turbo\",  # 모델명\n",
    "# )\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever=retriever,\n",
    "    # chain_type=\"stuff\", \n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "query_list = [\"마크앤컴퍼니의 상호, 상호의 영어명, 본점, 액면가, 발행한 주식의 총수, 발행할 주식의 총수, 회사성립연월일, 등기번호, 등록번호에 대해 표형식으로 정리해줘\"]\n",
    "for query in query_list:\n",
    "    ## open ai\n",
    "    # result_open = conv_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "    # print(result_open)\n",
    "    # chat_history.append((query, result_open[\"answer\"]))\n",
    "    \n",
    "    ##  hugging face\n",
    "    result = conv_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "    print()\n",
    "    chat_history.append((query, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the par value of Mark & Company Inc.?</s>\n",
      "Answer: 마크앤컴퍼니(Mark & Company Inc.)의 액면가액은 500원입니다.</s>{'question': '마크앤컴퍼니의 액면가는?', 'chat_history': [HumanMessage(content='마크앤컴퍼니의 상호, 상호의 영어명, 본점, 액면가, 발행한 주식의 총수, 발행할 주식의 총수, 회사성립연월일, 등기번호, 등록번호에 대해 표형식으로 정리해줘'), AIMessage(content='\\nAnswer: | 항목 | 내용 |\\n| --- | --- |\\n| 상호 | 마크앤컴퍼니 |\\n| 영어명 | Mark & Company Inc. |\\n| 본점 | 서울특별시 서초구 강남대로 311, 705호(서초동, 드림플러스 강남) |\\n| 액면가 | 금 500 원 (2019.06.10 변경) |\\n| 발행한 주식의 총수 | 240,000 주 (2020.04.30 변경) |\\n| 발행할 주식의 총수 | 1,000,000 주 |\\n| 회사성립연월일 | 2019 년 03 월 11 일 |\\n| 등기번호 | 703850 |\\n| 등록번호 | 110111-7038501 |</s>'), HumanMessage(content='마크앤컴퍼니의 액면가는?'), AIMessage(content='\\nAnswer: 마크앤컴퍼니(Mark & Company Inc.)의 액면가액은 500원입니다.</s>')], 'answer': '\\nAnswer: 마크앤컴퍼니(Mark & Company Inc.)의 액면가액은 500원입니다.</s>'}\n"
     ]
    }
   ],
   "source": [
    "query = \"마크앤컴퍼니의 액면가는?\"\n",
    "result = conv_chain.invoke({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('마크앤컴퍼니의 상호, 상호의 영어명, 본점, 액면가, 발행한 주식의 총수, 발행할 주식의 총수, 회사성립연월일, 등기번호, 등록번호에 대해 표형식으로 정리해줘',\n",
       "  '\\nAnswer: | 항목 | 내용 |\\n| --- | --- |\\n| 상호 | 마크앤컴퍼니 |\\n| 영어명 | Mark & Company Inc. |\\n| 본점 | 서울특별시 서초구 강남대로 311, 705호(서초동, 드림플러스 강남) |\\n| 액면가 | 금 500 원 (2019.06.10 변경) |\\n| 발행한 주식의 총수 | 240,000 주 (2020.04.30 변경) |\\n| 발행할 주식의 총수 | 1,000,000 주 |\\n| 회사성립연월일 | 2019 년 03 월 11 일 |\\n| 등기번호 | 703850 |\\n| 등록번호 | 110111-7038501 |</s>')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
